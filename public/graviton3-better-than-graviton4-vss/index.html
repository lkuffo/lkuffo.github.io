<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>AWS Graviton 3 > Graviton 4 for Vector Similarity Search | Leonardo Kuffo</title>
<meta name=keywords content="simd,sve,graviton,neon,aws,vss,vector-search,vector-similarity-search,nearest-neighbor-search,avx512,avx"><meta name=description content="
TL;DR: If you are doing vector search with a vector library that supports SVE, you should use a Graviton 3 machine. It is cheaper, and it will also deliver more raw performance.
A few months ago, we started working on a vertical layout for vector similarity search (PDX). As part of the benchmarks that we were running on different microarchitectures and vector systems like FAISS, Milvus, and Usearch, there was an observation that puzzled us: Graviton3 performed better than Graviton4 in almost all vector search scenarios, not only in queries per dollar (QP$) but also in queries per second (QPS). This was the case across vector libraries and even in our implementations of vector search algorithms. Here is one example of the QPS and QP$ of both microarchitectures on queries to an IVF index on float32 vectors with FAISS compiled with SVE."><meta name=author content="Leonardo Kuffo"><link rel=canonical href=http://localhost:1313/graviton3-better-than-graviton4-vss/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/circular.png><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/circular.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/circular.png><link rel=apple-touch-icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/graviton3-better-than-graviton4-vss/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="http://localhost:1313/graviton3-better-than-graviton4-vss/"><meta property="og:site_name" content="Leonardo Kuffo"><meta property="og:title" content="AWS Graviton 3 > Graviton 4 for Vector Similarity Search"><meta property="og:description" content=" TL;DR: If you are doing vector search with a vector library that supports SVE, you should use a Graviton 3 machine. It is cheaper, and it will also deliver more raw performance.
A few months ago, we started working on a vertical layout for vector similarity search (PDX). As part of the benchmarks that we were running on different microarchitectures and vector systems like FAISS, Milvus, and Usearch, there was an observation that puzzled us: Graviton3 performed better than Graviton4 in almost all vector search scenarios, not only in queries per dollar (QP$) but also in queries per second (QPS). This was the case across vector libraries and even in our implementations of vector search algorithms. Here is one example of the QPS and QP$ of both microarchitectures on queries to an IVF index on float32 vectors with FAISS compiled with SVE."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-03-20T12:00:00+00:00"><meta property="article:modified_time" content="2025-03-20T12:00:00+00:00"><meta property="article:tag" content="Simd"><meta property="article:tag" content="Sve"><meta property="article:tag" content="Graviton"><meta property="article:tag" content="Neon"><meta property="article:tag" content="Aws"><meta property="article:tag" content="Vss"><meta property="og:image" content="http://localhost:1313/graviton3-better-than-graviton4-vss/g3-vs-g4/gravitons.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/graviton3-better-than-graviton4-vss/g3-vs-g4/gravitons.png"><meta name=twitter:title content="AWS Graviton 3 > Graviton 4 for Vector Similarity Search"><meta name=twitter:description content="
TL;DR: If you are doing vector search with a vector library that supports SVE, you should use a Graviton 3 machine. It is cheaper, and it will also deliver more raw performance.
A few months ago, we started working on a vertical layout for vector similarity search (PDX). As part of the benchmarks that we were running on different microarchitectures and vector systems like FAISS, Milvus, and Usearch, there was an observation that puzzled us: Graviton3 performed better than Graviton4 in almost all vector search scenarios, not only in queries per dollar (QP$) but also in queries per second (QPS). This was the case across vector libraries and even in our implementations of vector search algorithms. Here is one example of the QPS and QP$ of both microarchitectures on queries to an IVF index on float32 vectors with FAISS compiled with SVE."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"AWS Graviton 3 \u003e Graviton 4 for Vector Similarity Search","item":"http://localhost:1313/graviton3-better-than-graviton4-vss/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"AWS Graviton 3 \u003e Graviton 4 for Vector Similarity Search","name":"AWS Graviton 3 \u003e Graviton 4 for Vector Similarity Search","description":" TL;DR: If you are doing vector search with a vector library that supports SVE, you should use a Graviton 3 machine. It is cheaper, and it will also deliver more raw performance.\nA few months ago, we started working on a vertical layout for vector similarity search (PDX). As part of the benchmarks that we were running on different microarchitectures and vector systems like FAISS, Milvus, and Usearch, there was an observation that puzzled us: Graviton3 performed better than Graviton4 in almost all vector search scenarios, not only in queries per dollar (QP$) but also in queries per second (QPS). This was the case across vector libraries and even in our implementations of vector search algorithms. Here is one example of the QPS and QP$ of both microarchitectures on queries to an IVF index on float32 vectors with FAISS compiled with SVE.\n","keywords":["simd","sve","graviton","neon","aws","vss","vector-search","vector-similarity-search","nearest-neighbor-search","avx512","avx"],"articleBody":" TL;DR: If you are doing vector search with a vector library that supports SVE, you should use a Graviton 3 machine. It is cheaper, and it will also deliver more raw performance.\nA few months ago, we started working on a vertical layout for vector similarity search (PDX). As part of the benchmarks that we were running on different microarchitectures and vector systems like FAISS, Milvus, and Usearch, there was an observation that puzzled us: Graviton3 performed better than Graviton4 in almost all vector search scenarios, not only in queries per dollar (QP$) but also in queries per second (QPS). This was the case across vector libraries and even in our implementations of vector search algorithms. Here is one example of the QPS and QP$ of both microarchitectures on queries to an IVF index on float32 vectors with FAISS compiled with SVE.\nR@10:\u003e0.99 OpenAI/1536 ----------------------------------- Graviton3 | 27.4 QPS ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà Graviton4 | 24.2 QPS ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà Graviton3 | 23.0 QP$ ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà Graviton4 | 18.4 QP$ ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà SIFT/128 -------------------------------------- Graviton3 | 557.1 QPS ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà Graviton4 | 538.2 QPS ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà Graviton3 | 468.1 QP$ ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà Graviton4 | 411.0 QP$ ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà (QP$ are in the order of 10^4) In the OpenAI/1536 dataset with 1M vectors, Graviton3 delivers 25% more queries per dollar than Graviton4!\nLet‚Äôs be clear: Graviton4 is a better machine than Graviton3. It has a higher clock frequency, a 2x bigger L2 cache, a slightly bigger L3, less memory-access latency, and a much more capable CPU (upgrading from Neoverse v1 to Neoverse v2). This is shown not only by AWS but also by innumerable benchmarks. I can refer to the benchmarks done by Daniel Lemire, Phoronix, and Chips and Cheese. But then, why would Graviton3 be better than Graviton4 on Vector Similarity Search?\nThe main culprit is that Graviton4 has a SVE SIMD register size of 128 bits ‚Äîhalf of the 256-bit registers of Graviton3.\nIn the rest of this blog post, we will dive deep into why this difference is particularly detrimental to the performance of vector similarity search and why this hasn‚Äôt been picked up by other benchmarks. But before discussing about the Gravitons, let‚Äôs refresh our knowledge of SIMD.\nSIMD in Vector Similarity Search Distance calculations in vector similarity search can be optimized using Single-Instruction-Multiple-Data (SIMD) instructions available in the CPU. These special instructions can process multiple values in parallel with a single CPU instruction.\nIn x86_64 architectures, SIMD instructions are called Advanced Vector Extensions (AVX). The number of values AVX can process at a time depends on the SIMD register width supported by the CPU. Initially, registers of 128-bit were introduced, further expanded to 256-bit (AVX2, used in Zen3 and Intel Sky Lake), and finally to 512-bit (AVX512, used in Zen4, Intel Ice Lake, Intel Sapphire Rapids), which can process 16 float32 values with one instruction.\nLet‚Äôs look at the following C++ code (taken from the SimSIMD codebase) that uses AVX512 SIMD to calculate the Euclidean distance (L2) between two vectors:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 void l2sq_f32_avx512(float const *a, float const *b, size_t n, float *result) { __m512 d2_vec = _mm512_setzero(); __m512 a_vec, b_vec; l2sq_f32_loop: a_vec = _mm512_loadu_ps(a); b_vec = _mm512_loadu_ps(b); a += 16, b += 16, n -= 16; __m512 d_vec = _mm512_sub_ps(a_vec, b_vec); d2_vec = _mm512_fmadd_ps(d_vec, d_vec, d2_vec); if (n) goto l2sq_f32_loop; *result = reduce_f32x16(d2_vec); } The critical part is the l2sq_f32_loop that loops through the vector dimensions. In each iteration, _mm512_loadu_ps loads 16 packed single-precision values (32 bits each) into the SIMD register. We do this twice, once for vector a and another for vector b. Then, we do the L2 calculation by doing a subtraction (_mm512_sub_ps) and a fused multiply-add (_mm512_fmadd_ps) that accumulates the distances into a result register (d2_vec). We keep repeating the loop until we have inspected all the vector dimensions (when n == 0). Finally, we have to sum all the values in the SIMD register to get our total distance (reduce_f32x16). We will obviate the explanation of this last step.\nOn the other hand, ARM architectures also provide SIMD instructions through NEON and SVE. NEON was introduced first, supporting SIMD over 128-bit registers (fitting 4 float32 values at a time). SVE was introduced later. Unlike AVX and NEON, SVE supports variable-size SIMD registers on its intrinsics through VLA (Variable Length Agnostic) programming. The latter alleviates technical debt as distance kernels no longer need hardware-dependent loop lengths.\nLet‚Äôs take a look now at a code with SVE intrinsics (also taken from SimSIMD):\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 void l2sq_f32_sve(float const *a, float const *b, size_t n, float *result) { size_t i = 0; svfloat32_t d2_vec = svdupq_n_f32(0.f, 0.f, 0.f, 0.f); do { svbool_t pg_vec = svwhilelt_b32((unsigned int)i, (unsigned int)n); svfloat32_t a_vec = svld1_f32(pg_vec, a + i); svfloat32_t b_vec = svld1_f32(pg_vec, b + i); svfloat32_t a_minus_b_vec = svsub_f32_x(pg_vec, a_vec, b_vec); d2_vec = svmla_f32_x(pg_vec, d2_vec, a_minus_b_vec, a_minus_b_vec); i += svcntw(); } while (i \u003c n); float d2 = svaddv_f32(svptrue_b32(), d2_vec); *result = d2; } svld1_f32 is the SIMD intrinsic that loads the single-precision vector into the SIMD register. svsub_f32_x and svmla_f32_x do the subtraction and fused multiply-add, resp. The main difference between SVE and AVX512 is that each loop iteration length is not controlled with a constant but with another intrinsic (svcntw()) that resolves to the register width of the underlying CPU. Recall that in the AVX512 code, we do n-=16 to advance through the loop. SVE has additional intricacies. For example, every intrinsic call must be predicated/masked. But we will not dive deep into SVE programming.\nBack to the Gravitons: From 256-bit to 128-bit SVE registers Both Gravitons support NEON and SVE SIMD. In terms of NEON, both microarchitectures have 128-bit SIMD registers. However, in terms of SVE, Graviton4 has 128-bit registers, while Graviton3 has 256-bit registers.\nA smaller SIMD register does not mean slower performance. Yes, every instruction call will process fewer values, but the performance also depends on the execution throughput and latencies of the used instructions. The execution throughput is defined on ARM guides as ‚Äúthe maximum number of instructions per CPU cycle an instruction can achieve.‚Äù The latter depends on the CPU design and the ports in which CPU instructions are dispatched. On the other hand, latency is defined as ‚Äúthe delay (in clock cycles) that the instruction generates in a dependency chain.‚Äù\nLet‚Äôs compare both microarchitectures execution throughput and latencies on the relevant instructions for our L2 distance kernel: FMADD and LOAD.\nExecution Throughput The first number in each cell is the execution throughput. To translate this to effective throughput of data, we multiply it by the size of the register that executes that instruction.\nMicroarchitecture NEON FMADD SVE FMADD NEON LOAD SVE LOAD Graviton 3 (Neoverse v1) 4 x 128 = 512 2 x 256 = 512 3 x 128 = 384 2 x 256 = 512 Graviton 4 (Neoverse v2) 4 x 128 = 512 4 x 128 = 512 3 x 128 = 384 3 x 128 = 384 *These numbers are taken from the official microarchitecture guides of Neoverse v1 and Neoverse v2\nWe see that Graviton4 maintains the execution throughput of floating-point arithmetic despite the smaller register width. From these numbers, only one stands out: the SVE LOAD. Graviton3 can load 33% more data than Graviton4 in one CPU cycle, which is more than the upgrade in clock frequency from Graviton3 to Graviton4 (around 8%). This gives an advantage to Graviton3 if the data is cache resident. In fact, this is reflected in our read memory bandwidth benchmarks that show that when data is L1-resident, the read bandwidth using SVE intrinsics is 26% higher in Graviton3.\nSVE Read Memory Bandwidth on Graviton3 (r7g.metal) and Graviton4 (r8g.metal-24xl)\nHowever, in vector search, we are usually on the case in which data is in L3/DRAM (in IVF indexes or full scans) or, at best, in L2 (e.g., in the top layer of HNSW indexes). Here, the difference in read bandwidth is small.\nLatencies Microarchitecture NEON FMADD SVE FMADD NEON LOAD SVE LOAD Graviton 3 (Neoverse v1) 4 4 4 6 Graviton 4 (Neoverse v2) 4 4 4 6 *These numbers are taken from the official microarchitecture guides of Neoverse v1 and Neoverse v2\nThe latencies are the same in both CPUs. However, the total latency cost to load the same amount of data is higher in Graviton4 due to the smaller register width. For example, the latency cost of calling FMADD in Graviton4 to process 128 bits (4 float32 values) is 4 cycles. However, in Graviton3, we spend the same 4 cycles to process 256 bits (8 float32 values).\nAlso, recall that in our AVX and SVE code of the L2 distance kernel, each iteration of the loop depends on the previous one since all the FMADDs are accumulating distances on the same SIMD register. This creates a dependency chain, making it harder for the CPU to leverage features such as out-of-order execution. Therefore, the SIMD register width becomes more critical in similarity calculations, as instructions may not be able to be executed in parallel up to their maximum throughput.\nOf course, it is hard to precisely determine the impact of these extra cycles and less effective throughput on the bigger picture of a vector similarity search query. Each CPU microarchitecture is wildly different and implements different mechanisms to maintain performance despite having a smaller register width. Therefore, let‚Äôs run some benchmarks!\nBenchmarks: Graviton 3 vs Graviton 4 We compared the queries per second (QPS) and queries per dollar (QP$) given by Graviton3 (r7g.2xlarge, $0.4284/h in us-east-1) and Graviton4 (r8g.2xlarge, $0.4713/h) on a variety of vector search scenarios. The machines have Ubuntu 24 with GCC 13 and LLVM 18.\nWe used 2 datasets contrasting in their dimensionality:\nOpenAI (D=1536, N=1M) SIFT (D=128, N=1M) For these single-threaded benchmarks, we used FAISS (compiled with SVE) and USearch.\nIVF Indexes in FAISS Graviton3 performs better at all recall levels and even more so in the dataset of higher dimensionality. Things look worse for Graviton4 when we consider its price, as Graviton3 is a cheaper machine (around 10% cheaper).\nNotice how the gap closes on the dataset with a lower dimensionality. However, it is nowhere near the 30% performance improvement AWS promises when jumping from Graviton3 to Graviton4.\nHNSW Indexes in USearch On float32, again, Graviton3 performs better at all recall levels. Graviton3 delivers 5% more QPS and 15% more QP$ in the OpenAI dataset at the highest recall level. And 13% and 25% more QPS and QPS, resp., in the SIFT/128 dataset.\nThe story is the same for quantized vectors. Here, we show only the performance at the highest possible recall level:\nOn the OpenAI/1536 dataset\nOn the SIFT/128 dataset\nIn the dataset of smaller dimensionality (SIFT/128), G4 takes the upper hand on QPS, but G3 remains competitive on QP$. Here, the bigger L2 of G4 could be kicking in due to the entry nodes of the HNSW index being cached more efficiently. Also, smaller vectors imply fewer calls to SIMD instructions, which benefits G4.\nNote 1: USearch switches to NEON for 1-bit vectors if the vectors are of 128 dimensions, which is the case here.\nNote 2: We did not benchmark quantized vectors in FAISS because FAISS does asymmetric distance calculations. This in itself would not be a problem, but for ARM, FAISS does not use SIMD instructions to go from the 8-bit, 6-bit, 4-bit domain to the float32 domain. This leads to poor performance in both architectures (\u003e6x slower than Zen4).\nRaw Distance Calculations (1-to-many) We ran a standalone benchmark of L2 distance calculations to eliminate possible artifacts and overhead introduced by vector systems. Here, we used randomly generated float32 collections of different sizes and dimensionalities. The collections range from being small enough to fit in L1 and large enough to spill to DRAM. Our code is as simple as it can get: Put the vectors in memory and do 1-to-many distance calculations with the L2 kernels taken from SimSIMD. Here, we do not do a KNN search; we only do pure distance calculations. We repeat this experiment thousands of times to warm up the caches.\nNEON vs NEON\nWhen using NEON kernels, Graviton4 is, on average, 10% faster than Graviton3 across all settings. This improvement is on par with the increase in clock frequency between both microarchitectures.\nSVE vs SVE\nWhen switching from NEON to SVE, Graviton3 saw a 37% performance improvement over its NEON counterpart. Ash Vardanian also reported similar findings. However, Graviton4 doesn‚Äôt find any benefits when using SVE (in fact, compiling FAISS to SVE or not yields the same performance). Actually, SVE is slightly slower than NEON in G4. These could be fluctuations/noise of the benchmarks or the overhead of masked operations in SVE.\nWhen comparing G3 SVE vs G4 SVE, these are the results across scenarios:\nGraviton3 is, on average, 31% faster than Graviton4.\nYou can check these fully disaggregated benchmarks for each collection size and dimensionality on this spreadsheet. We would like to bring forward a few things regarding these benchmarks: (i) The wider the vectors, the bigger the gap between G3 and G4. (ii) If the vectors fit in the cache, Graviton3 can be almost 2x faster than Graviton4. (iii) Only at a dimensionality of 8, the tables turn, and Graviton3 is slightly slower than Graviton4.\nThe benchmarks are clear: if you are doing vector search with a vector library that supports SVE, you should use Graviton 3. It is cheaper, and it will also deliver more raw performance in the majority of scenarios.\nWhy did Graviton4 regress the SVE register width? While we do not have a proper answer to this question, we can speculate.\nCurrently, most code for ARM is written in NEON, where Graviton4 is better than Graviton3. A possibility is that AWS went for a CPU that performed better in existing workloads and benchmarks, which usually use code written in NEON. In fact, Daniel Lemire and Chips and Cheese benchmarks on Graviton4 vs Graviton3 used NEON code. In other words, SVE is not yet widely used.\nAnother possible reason is that SIMD instructions can be fully pipelined in many workloads. However, similarity calculations are different because there is a dependency chain between multiple calls to SIMD instructions. The latter benefits wider registers, especially on vectors of high dimensionalities. While we haven‚Äôt done benchmarks on SVE for other types of workloads, our intuition is that if the workload can be fully pipelined, it would be faster in Graviton4.\nClosing Remarks I have found myself inside a (nice) rabbit hole of CPU microarchitecture design and performance. We have actually done the same experiments presented in this blog post for 5 microarchitectures (Intel SPR, Graviton3, Graviton4, Zen3, and Zen4). One of our most interesting findings is that carefully choosing the microarchitecture for your vector search use case can give you up to 3x more queries per second and queries per dollar. This is the case, for instance, with Zen4 in IVF indexes compared to Intel Sapphire Rapids (despite the latter being a CPU with better specs). I am writing a blog post about that, so keep tuned!\nRegarding the Gravitons, it is nonetheless a weird decision to halve the SIMD register size in the generational jump from Graviton3 to Graviton4. Perhaps in terms of the engineering design of the core, it is hard to keep supporting NEON registers of 128 bits AND SVE registers of double the size.\nNot so far ago, Daniel Lemire commented that ‚ÄúAMD is mopping the floor with ARM in terms of SIMD.‚Äù To my knowledge, the best NEON offers is 4 x 128-bit, so four 128-bit instructions per cycle.\nAMD is at 4 x 512-bit... and that's AVX-512, so much more powerful than NEON.\nSo on SIMD... AMD is just mopping the floor with ARM.\nARM has SVE/SVE2 but that does not seem to be going‚Ä¶ pic.twitter.com/9AvOQW8lzl\n‚Äî Daniel Lemire (@lemire) February 11, 2025 It is no rocket science: a smaller register will impact the performance of some workloads, even if the execution throughput is kept the same. Of course, when comparing different microarchitectures, more things come into play, such as memory access latency, read memory bandwidth, and the data-access patterns of the workload. Ultimately, your decision to use a microarchitecture should be based on data-driven benchmarks with your own use case. As you may have noticed, the performance depends on various factors, such as the search algorithm and the size of the vectors. Perhaps an interesting follow-up post would be to test both architectures by doing vector search under a multi-threaded setting.\nFor now, it seems that aside from the portability benefits, there is currently not much payoff in migrating NEON code to SVE, especially if the cores used by AWS will keep the SVE register size on par with NEON. The only exception would be when one needs to use an intrinsic that is only available in SVE.\nKudos to Ash for giving input on these findings and for putting them forward to the ARM team. We are still awaiting their input.\n","wordCount":"2850","inLanguage":"en","image":"http://localhost:1313/graviton3-better-than-graviton4-vss/g3-vs-g4/gravitons.png","datePublished":"2025-03-20T12:00:00Z","dateModified":"2025-03-20T12:00:00Z","author":{"@type":"Person","name":"Leonardo Kuffo"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/graviton3-better-than-graviton4-vss/"},"publisher":{"@type":"Organization","name":"Leonardo Kuffo","logo":{"@type":"ImageObject","url":"http://localhost:1313/circular.png"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="lkuffo (Alt + H)"><img src=http://localhost:1313/circular.png alt aria-label=logo height=25>lkuffo</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/archives/ title="üìù Posts"><span>üìù Posts</span></a></li><li><a href=http://localhost:1313/publications/ title="üìô My Research"><span>üìô My Research</span></a></li><li><a href=http://localhost:1313/tags/ title=Tags><span>Tags</span></a></li><li><a href=http://localhost:1313/lk_cv_2025.pdf title=CV><span>CV</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;¬ª&nbsp;<a href=http://localhost:1313/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">AWS Graviton 3 > Graviton 4 for Vector Similarity Search</h1><div class=post-meta><span title='2025-03-20 12:00:00 +0000 UTC'>March 20, 2025</span>&nbsp;¬∑&nbsp;14 min&nbsp;¬∑&nbsp;2850 words&nbsp;¬∑&nbsp;Leonardo Kuffo</div></header><figure class=entry-cover><img loading=eager src=http://localhost:1313/g3-vs-g4/gravitons.png alt="Graviton3 vs Graviton4 in Vector Similarity Search"></figure><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#simd-in-vector-similarity-search>SIMD in Vector Similarity Search</a></li><li><a href=#back-to-the-gravitons-from-256-bit-to-128-bit-sve-registers>Back to the Gravitons: From 256-bit to 128-bit SVE registers</a><ul><li><a href=#execution-throughput>Execution Throughput</a></li><li><a href=#latencies>Latencies</a></li></ul></li><li><a href=#benchmarks-graviton-3-vs-graviton-4>Benchmarks: Graviton 3 vs Graviton 4</a><ul><li><a href=#ivf-indexes-in-faiss>IVF Indexes in FAISS</a></li><li><a href=#hnsw-indexes-in-usearch>HNSW Indexes in USearch</a></li><li><a href=#raw-distance-calculations-1-to-many>Raw Distance Calculations (1-to-many)</a></li></ul></li><li><a href=#why-did-graviton4-regress-the-sve-register-width>Why did Graviton4 regress the SVE register width?</a></li><li><a href=#closing-remarks>Closing Remarks</a></li></ul></nav></div></details></div><div class=post-content><blockquote><p>TL;DR: <strong>If you are doing vector search with a vector library that supports SVE, you should use a Graviton 3 machine. It is cheaper, and it will also deliver more raw performance.</strong></p></blockquote><p>A few months ago, we started working on a vertical layout for vector similarity search (<a href=https://github.com/cwida/PDX>PDX</a>). As part of the benchmarks that we were running on different microarchitectures and vector systems like FAISS, Milvus, and Usearch, there was an observation that puzzled us: <strong>Graviton3 performed better than Graviton4 in almost all vector search scenarios</strong>, not only in queries per dollar (QP$) but also in queries per second (QPS). This was the case across vector libraries and even in our implementations of vector search algorithms. Here is one example of the QPS and QP$ of both microarchitectures on queries to an IVF index on <code>float32</code> vectors with FAISS compiled with SVE.</p><pre tabindex=0><code>R@10:&gt;0.99
OpenAI/1536 -----------------------------------
Graviton3  | 27.4 QPS ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
Graviton4  | 24.2 QPS ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà

Graviton3  | 23.0 QP$ ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
Graviton4  | 18.4 QP$ ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà

SIFT/128 --------------------------------------
Graviton3 | 557.1 QPS ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
Graviton4 | 538.2 QPS ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà

Graviton3 | 468.1 QP$ ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
Graviton4 | 411.0 QP$ ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà

(QP$ are in the order of 10^4)
</code></pre><p>In the OpenAI/1536 dataset with 1M vectors, Graviton3 delivers 25% more queries per dollar than Graviton4!</p><p><strong>Let‚Äôs be clear:</strong> Graviton4 is a better machine than Graviton3. It has a higher clock frequency, a 2x bigger L2 cache, a slightly bigger L3, less memory-access latency, and a much more capable CPU (upgrading from Neoverse v1 to Neoverse v2). This is shown not only by AWS but also by innumerable benchmarks. I can refer to the benchmarks done by <a href=https://lemire.me/blog/2024/07/10/benchmarking-arm-processors-graviton-4-graviton-3-and-apple-m2/>Daniel Lemire</a>, <a href=https://www.phoronix.com/review/aws-graviton4-benchmarks>Phoronix</a>, and <a href=https://chipsandcheese.com/p/arms-neoverse-v2-in-awss-graviton-4>Chips and Cheese</a>. But then, <em>why would Graviton3 be better than Graviton4 on Vector Similarity Search?</em></p><p><strong>The main culprit</strong> is that Graviton4 has a SVE SIMD register size of 128 bits <em><strong>‚Äîhalf of the 256-bit registers of Graviton3</strong></em>.</p><p>In the rest of this blog post, we will dive deep into <em>why</em> this difference is particularly detrimental to the performance of vector similarity search and <em>why</em> this hasn‚Äôt been picked up by other benchmarks. But before discussing about the Gravitons, let&rsquo;s refresh our knowledge of SIMD.</p><h2 id=simd-in-vector-similarity-search>SIMD in Vector Similarity Search<a hidden class=anchor aria-hidden=true href=#simd-in-vector-similarity-search>#</a></h2><p>Distance calculations in vector similarity search can be optimized using Single-Instruction-Multiple-Data (SIMD) instructions available in the CPU. These special instructions can process <strong>multiple values in parallel with a single CPU instruction</strong>.</p><p>In <strong>x86_64 architectures</strong>, SIMD instructions are called Advanced Vector Extensions (AVX). The number of values AVX can process at a time depends on the SIMD register width supported by the CPU. Initially, registers of 128-bit were introduced, further expanded to 256-bit (AVX2, used in Zen3 and Intel Sky Lake), and finally to 512-bit (AVX512, used in Zen4, Intel Ice Lake, Intel Sapphire Rapids), which can process 16 <code>float32</code> values with one instruction.</p><p>Let‚Äôs look at the following C++ code (taken from the <a href=https://github.com/ashvardanian/SimSIMD/blob/6951b006d8c27b89c91b78019c7af3714b7114f5/include/simsimd/spatial.h#L1520>SimSIMD</a> codebase) that uses AVX512 SIMD to calculate the Euclidean distance (L2) between two vectors:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-1-1><a class=lnlinks href=#hl-1-1> 1</a>
</span><span class=lnt id=hl-1-2><a class=lnlinks href=#hl-1-2> 2</a>
</span><span class=lnt id=hl-1-3><a class=lnlinks href=#hl-1-3> 3</a>
</span><span class=lnt id=hl-1-4><a class=lnlinks href=#hl-1-4> 4</a>
</span><span class=lnt id=hl-1-5><a class=lnlinks href=#hl-1-5> 5</a>
</span><span class=lnt id=hl-1-6><a class=lnlinks href=#hl-1-6> 6</a>
</span><span class=lnt id=hl-1-7><a class=lnlinks href=#hl-1-7> 7</a>
</span><span class=lnt id=hl-1-8><a class=lnlinks href=#hl-1-8> 8</a>
</span><span class=lnt id=hl-1-9><a class=lnlinks href=#hl-1-9> 9</a>
</span><span class=lnt id=hl-1-10><a class=lnlinks href=#hl-1-10>10</a>
</span><span class=lnt id=hl-1-11><a class=lnlinks href=#hl-1-11>11</a>
</span><span class=lnt id=hl-1-12><a class=lnlinks href=#hl-1-12>12</a>
</span><span class=lnt id=hl-1-13><a class=lnlinks href=#hl-1-13>13</a>
</span><span class=lnt id=hl-1-14><a class=lnlinks href=#hl-1-14>14</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-c data-lang=c><span class=line><span class=cl><span class=kt>void</span> <span class=nf>l2sq_f32_avx512</span><span class=p>(</span><span class=kt>float</span> <span class=k>const</span> <span class=o>*</span><span class=n>a</span><span class=p>,</span> <span class=kt>float</span> <span class=k>const</span> <span class=o>*</span><span class=n>b</span><span class=p>,</span> <span class=kt>size_t</span> <span class=n>n</span><span class=p>,</span> <span class=kt>float</span> <span class=o>*</span><span class=n>result</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>__m512</span> <span class=n>d2_vec</span> <span class=o>=</span> <span class=nf>_mm512_setzero</span><span class=p>();</span>
</span></span><span class=line><span class=cl>    <span class=n>__m512</span> <span class=n>a_vec</span><span class=p>,</span> <span class=n>b_vec</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nl>l2sq_f32_loop</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>a_vec</span> <span class=o>=</span> <span class=nf>_mm512_loadu_ps</span><span class=p>(</span><span class=n>a</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=n>b_vec</span> <span class=o>=</span> <span class=nf>_mm512_loadu_ps</span><span class=p>(</span><span class=n>b</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=n>a</span> <span class=o>+=</span> <span class=mi>16</span><span class=p>,</span> <span class=n>b</span> <span class=o>+=</span> <span class=mi>16</span><span class=p>,</span> <span class=n>n</span> <span class=o>-=</span> <span class=mi>16</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=n>__m512</span> <span class=n>d_vec</span> <span class=o>=</span> <span class=nf>_mm512_sub_ps</span><span class=p>(</span><span class=n>a_vec</span><span class=p>,</span> <span class=n>b_vec</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=n>d2_vec</span> <span class=o>=</span> <span class=nf>_mm512_fmadd_ps</span><span class=p>(</span><span class=n>d_vec</span><span class=p>,</span> <span class=n>d_vec</span><span class=p>,</span> <span class=n>d2_vec</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=p>(</span><span class=n>n</span><span class=p>)</span> <span class=k>goto</span> <span class=n>l2sq_f32_loop</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=o>*</span><span class=n>result</span> <span class=o>=</span> <span class=nf>reduce_f32x16</span><span class=p>(</span><span class=n>d2_vec</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><p>The critical part is the <code>l2sq_f32_loop</code> that loops through the vector dimensions. In each iteration, <code>_mm512_loadu_ps</code> loads 16 packed single-precision values (32 bits each) into the SIMD register. We do this twice, once for vector <code>a</code> and another for vector <code>b</code>. Then, we do the L2 calculation by doing a subtraction (<code>_mm512_sub_ps</code>) and a fused multiply-add (<code>_mm512_fmadd_ps</code>) that accumulates the distances into a result register (<code>d2_vec</code>). We keep repeating the loop until we have inspected all the vector dimensions (when <code>n == 0</code>). Finally, we have to sum all the values in the SIMD register to get our total distance (<code>reduce_f32x16</code>). We will obviate the explanation of this last step.</p><p>On the other hand, <strong>ARM architectures</strong> also provide SIMD instructions through NEON and SVE. NEON was introduced first, supporting SIMD over 128-bit registers (fitting 4 <code>float32</code> values at a time). SVE was introduced later. Unlike AVX and NEON, SVE supports variable-size SIMD registers on its intrinsics through VLA (Variable Length Agnostic) programming. The latter alleviates technical debt as distance kernels no longer need hardware-dependent loop lengths.</p><p>Let‚Äôs take a look now at a code with SVE intrinsics (also taken from <a href=https://github.com/ashvardanian/SimSIMD/blob/6951b006d8c27b89c91b78019c7af3714b7114f5/include/simsimd/spatial.h#L799>SimSIMD</a>):</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-2-1><a class=lnlinks href=#hl-2-1> 1</a>
</span><span class=lnt id=hl-2-2><a class=lnlinks href=#hl-2-2> 2</a>
</span><span class=lnt id=hl-2-3><a class=lnlinks href=#hl-2-3> 3</a>
</span><span class=lnt id=hl-2-4><a class=lnlinks href=#hl-2-4> 4</a>
</span><span class=lnt id=hl-2-5><a class=lnlinks href=#hl-2-5> 5</a>
</span><span class=lnt id=hl-2-6><a class=lnlinks href=#hl-2-6> 6</a>
</span><span class=lnt id=hl-2-7><a class=lnlinks href=#hl-2-7> 7</a>
</span><span class=lnt id=hl-2-8><a class=lnlinks href=#hl-2-8> 8</a>
</span><span class=lnt id=hl-2-9><a class=lnlinks href=#hl-2-9> 9</a>
</span><span class=lnt id=hl-2-10><a class=lnlinks href=#hl-2-10>10</a>
</span><span class=lnt id=hl-2-11><a class=lnlinks href=#hl-2-11>11</a>
</span><span class=lnt id=hl-2-12><a class=lnlinks href=#hl-2-12>12</a>
</span><span class=lnt id=hl-2-13><a class=lnlinks href=#hl-2-13>13</a>
</span><span class=lnt id=hl-2-14><a class=lnlinks href=#hl-2-14>14</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-c data-lang=c><span class=line><span class=cl><span class=kt>void</span> <span class=nf>l2sq_f32_sve</span><span class=p>(</span><span class=kt>float</span> <span class=k>const</span> <span class=o>*</span><span class=n>a</span><span class=p>,</span> <span class=kt>float</span> <span class=k>const</span> <span class=o>*</span><span class=n>b</span><span class=p>,</span> <span class=kt>size_t</span> <span class=n>n</span><span class=p>,</span> <span class=kt>float</span> <span class=o>*</span><span class=n>result</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=kt>size_t</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=kt>svfloat32_t</span> <span class=n>d2_vec</span> <span class=o>=</span> <span class=nf>svdupq_n_f32</span><span class=p>(</span><span class=mf>0.f</span><span class=p>,</span> <span class=mf>0.f</span><span class=p>,</span> <span class=mf>0.f</span><span class=p>,</span> <span class=mf>0.f</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=k>do</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=kt>svbool_t</span> <span class=n>pg_vec</span> <span class=o>=</span> <span class=nf>svwhilelt_b32</span><span class=p>((</span><span class=kt>unsigned</span> <span class=kt>int</span><span class=p>)</span><span class=n>i</span><span class=p>,</span> <span class=p>(</span><span class=kt>unsigned</span> <span class=kt>int</span><span class=p>)</span><span class=n>n</span><span class=p>);</span>
</span></span><span class=line><span class=cl>        <span class=kt>svfloat32_t</span> <span class=n>a_vec</span> <span class=o>=</span> <span class=nf>svld1_f32</span><span class=p>(</span><span class=n>pg_vec</span><span class=p>,</span> <span class=n>a</span> <span class=o>+</span> <span class=n>i</span><span class=p>);</span>
</span></span><span class=line><span class=cl>        <span class=kt>svfloat32_t</span> <span class=n>b_vec</span> <span class=o>=</span> <span class=nf>svld1_f32</span><span class=p>(</span><span class=n>pg_vec</span><span class=p>,</span> <span class=n>b</span> <span class=o>+</span> <span class=n>i</span><span class=p>);</span>
</span></span><span class=line><span class=cl>        <span class=kt>svfloat32_t</span> <span class=n>a_minus_b_vec</span> <span class=o>=</span> <span class=nf>svsub_f32_x</span><span class=p>(</span><span class=n>pg_vec</span><span class=p>,</span> <span class=n>a_vec</span><span class=p>,</span> <span class=n>b_vec</span><span class=p>);</span>
</span></span><span class=line><span class=cl>        <span class=n>d2_vec</span> <span class=o>=</span> <span class=nf>svmla_f32_x</span><span class=p>(</span><span class=n>pg_vec</span><span class=p>,</span> <span class=n>d2_vec</span><span class=p>,</span> <span class=n>a_minus_b_vec</span><span class=p>,</span> <span class=n>a_minus_b_vec</span><span class=p>);</span>
</span></span><span class=line><span class=cl>        <span class=n>i</span> <span class=o>+=</span> <span class=nf>svcntw</span><span class=p>();</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span> <span class=k>while</span> <span class=p>(</span><span class=n>i</span> <span class=o>&lt;</span> <span class=n>n</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=kt>float</span> <span class=n>d2</span> <span class=o>=</span> <span class=nf>svaddv_f32</span><span class=p>(</span><span class=nf>svptrue_b32</span><span class=p>(),</span> <span class=n>d2_vec</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=o>*</span><span class=n>result</span> <span class=o>=</span> <span class=n>d2</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><p><code>svld1_f32</code> is the SIMD intrinsic that loads the single-precision vector into the SIMD register. <code>svsub_f32_x</code> and <code>svmla_f32_x</code> do the subtraction and fused multiply-add, resp. The main difference between SVE and AVX512 is that each loop iteration length is not controlled with a constant but with another intrinsic (<code>svcntw()</code>) that resolves to the register width of the underlying CPU. Recall that in the AVX512 code, we do <code>n-=16</code> to advance through the loop. SVE has additional intricacies. For example, every intrinsic call must be predicated/masked. But we will not dive deep into SVE programming.</p><h2 id=back-to-the-gravitons-from-256-bit-to-128-bit-sve-registers>Back to the Gravitons: From 256-bit to 128-bit SVE registers<a hidden class=anchor aria-hidden=true href=#back-to-the-gravitons-from-256-bit-to-128-bit-sve-registers>#</a></h2><p>Both Gravitons support NEON and SVE SIMD. In terms of NEON, both microarchitectures have 128-bit SIMD registers. However, in terms of SVE, Graviton4 has 128-bit registers, while Graviton3 has 256-bit registers.</p><p>A smaller SIMD register <em>does not</em> mean slower performance. Yes, every instruction call will process fewer values, but the performance also depends on the <strong>execution throughput</strong> and <strong>latencies</strong> of the used instructions. The <em>execution throughput</em> is defined on ARM guides as &ldquo;the maximum number of instructions per CPU cycle an instruction can achieve.&rdquo; The latter depends on the CPU design and the ports in which CPU instructions are dispatched. On the other hand, <em>latency</em> is defined as &ldquo;the delay (in clock cycles) that the instruction generates in a dependency chain.&rdquo;</p><p>Let‚Äôs compare both microarchitectures execution throughput and latencies on the relevant instructions for our L2 distance kernel: <code>FMADD</code> and <code>LOAD</code>.</p><h3 id=execution-throughput>Execution Throughput<a hidden class=anchor aria-hidden=true href=#execution-throughput>#</a></h3><p>The first number in each cell is the execution throughput. To translate this to effective throughput of <em>data</em>, we multiply it by the size of the register that executes that instruction.</p><table><thead><tr><th>Microarchitecture</th><th>NEON <code>FMADD</code></th><th>SVE <code>FMADD</code></th><th>NEON <code>LOAD</code></th><th>SVE <code>LOAD</code></th></tr></thead><tbody><tr><td>Graviton 3 (Neoverse v1)</td><td>4 x 128 = 512</td><td>2 x 256 = 512</td><td>3 x 128 = 384</td><td><strong>2 x 256 = 512</strong></td></tr><tr><td>Graviton 4 (Neoverse v2)</td><td>4 x 128 = 512</td><td>4 x 128 = 512</td><td>3 x 128 = 384</td><td>3 x 128 = 384</td></tr></tbody></table><p>*<em>These numbers are taken from the official microarchitecture guides of <a href=https://developer.arm.com/documentation/109897/0600>Neoverse v1</a> and <a href=https://developer.arm.com/documentation/109898/0300/>Neoverse v2</a></em></p><p>We see that Graviton4 maintains the execution throughput of floating-point arithmetic despite the smaller register width. From these numbers, only one stands out: the <strong>SVE LOAD</strong>. Graviton3 can load 33% more data than Graviton4 in one CPU cycle, which is more than the upgrade in clock frequency from Graviton3 to Graviton4 (around 8%). This gives an advantage to Graviton3 if the data is cache resident. In fact, this is reflected in our read memory bandwidth benchmarks that show that when data is L1-resident, the read bandwidth using SVE intrinsics is 26% higher in Graviton3.</p><figure class=align-center><img loading=lazy src=/g3-vs-g4/gravitons-memory_benchmarks.png#center alt="SVE Read Memory Bandwidth on Graviton3 (r7g.metal) and Graviton4 (r8g.metal-24xl)"><figcaption><p>SVE Read Memory Bandwidth on Graviton3 (r7g.metal) and Graviton4 (r8g.metal-24xl)</p></figcaption></figure><p>However, in vector search, we are usually on the case in which data is in L3/DRAM (in IVF indexes or full scans) or, at best, in L2 (e.g., in the top layer of HNSW indexes). Here, the difference in read bandwidth is small.</p><h3 id=latencies>Latencies<a hidden class=anchor aria-hidden=true href=#latencies>#</a></h3><table><thead><tr><th>Microarchitecture</th><th>NEON <code>FMADD</code></th><th>SVE <code>FMADD</code></th><th>NEON <code>LOAD</code></th><th>SVE <code>LOAD</code></th></tr></thead><tbody><tr><td>Graviton 3 (Neoverse v1)</td><td>4</td><td>4</td><td>4</td><td>6</td></tr><tr><td>Graviton 4 (Neoverse v2)</td><td>4</td><td>4</td><td>4</td><td>6</td></tr></tbody></table><p>*<em>These numbers are taken from the official microarchitecture guides of <a href=https://developer.arm.com/documentation/109897/0600>Neoverse v1</a> and <a href=https://developer.arm.com/documentation/109898/0300/>Neoverse v2</a></em></p><p>The latencies are the same in both CPUs. However, the total latency cost to load the same amount of data is higher in Graviton4 due to the smaller register width. For example, the latency cost of calling <code>FMADD</code> in Graviton4 to process 128 bits (4 <code>float32</code> values) is 4 cycles. However, in Graviton3, we spend the same 4 cycles to process 256 bits (8 <code>float32</code> values).</p><p>Also, recall that in our AVX and SVE code of the L2 distance kernel, each iteration of the loop depends on the previous one since all the FMADDs are accumulating distances on the same SIMD register. This creates a dependency chain, making it harder for the CPU to leverage features such as out-of-order execution. Therefore, <strong>the SIMD register width becomes more critical in similarity calculations</strong>, as instructions may not be able to be executed in parallel up to their maximum throughput.</p><p>Of course, it is hard to precisely determine the impact of these extra cycles and less effective throughput on the bigger picture of a vector similarity search query. Each CPU microarchitecture is wildly different and implements different mechanisms to maintain performance despite having a smaller register width. Therefore, let&rsquo;s run some benchmarks!</p><h2 id=benchmarks-graviton-3-vs-graviton-4>Benchmarks: Graviton 3 vs Graviton 4<a hidden class=anchor aria-hidden=true href=#benchmarks-graviton-3-vs-graviton-4>#</a></h2><p>We compared the queries per second (QPS) and queries per dollar (QP$) given by Graviton3 (<code>r7g.2xlarge</code>, $0.4284/h in <code>us-east-1</code>) and Graviton4 (<code>r8g.2xlarge</code>, $0.4713/h) on a variety of vector search scenarios. The machines have Ubuntu 24 with GCC 13 and LLVM 18.</p><p>We used 2 datasets contrasting in their dimensionality:</p><ul><li>OpenAI (D=1536, N=1M)</li><li>SIFT (D=128, N=1M)</li></ul><p>For these single-threaded benchmarks, we used <a href=https://github.com/facebookresearch/faiss>FAISS</a> (compiled with SVE) and <a href=https://github.com/unum-cloud/usearch>USearch</a>.</p><h3 id=ivf-indexes-in-faiss>IVF Indexes in FAISS<a hidden class=anchor aria-hidden=true href=#ivf-indexes-in-faiss>#</a></h3><p>Graviton3 performs better at all recall levels and even more so in the dataset of higher dimensionality. Things look worse for Graviton4 when we consider its price, as Graviton3 is a cheaper machine (around 10% cheaper).</p><figure class=align-center><img loading=lazy src=/g3-vs-g4/gravitons-ivf_flat_faiss.png#center></figure><p>Notice how the gap closes on the dataset with a lower dimensionality. However, it is nowhere near the 30% performance improvement AWS promises when jumping from Graviton3 to Graviton4.</p><h3 id=hnsw-indexes-in-usearch>HNSW Indexes in USearch<a hidden class=anchor aria-hidden=true href=#hnsw-indexes-in-usearch>#</a></h3><p>On <code>float32</code>, again, Graviton3 performs better at all recall levels. Graviton3 delivers 5% more QPS and 15% more QP$ in the OpenAI dataset at the highest recall level. And 13% and 25% more QPS and QPS, resp., in the SIFT/128 dataset.</p><figure class=align-center><img loading=lazy src=/g3-vs-g4/gravitons-hnsw_flat_usearch.png#center></figure><p>The story is the same for quantized vectors. Here, we show only the performance at the highest possible recall level:</p><figure class=align-center><img loading=lazy src=/g3-vs-g4/gravitons-hnsw-openai.png#center alt="On the OpenAI/1536 dataset"><figcaption><p>On the OpenAI/1536 dataset</p></figcaption></figure><figure class=align-center><img loading=lazy src=/g3-vs-g4/gravitons-hnsw-sift.png#center alt="On the SIFT/128 dataset"><figcaption><p>On the SIFT/128 dataset</p></figcaption></figure><p>In the dataset of smaller dimensionality (SIFT/128), G4 takes the upper hand on QPS, but G3 remains competitive on QP$. Here, the bigger L2 of G4 could be kicking in due to the entry nodes of the HNSW index being cached more efficiently. Also, smaller vectors imply fewer calls to SIMD instructions, which benefits G4.</p><p><strong>Note 1</strong>: USearch switches to NEON for 1-bit vectors if the vectors are of 128 dimensions, which is the case here.</p><p><strong>Note 2</strong>: We did not benchmark quantized vectors in FAISS because FAISS does asymmetric distance calculations. This in itself would not be a problem, but for ARM, FAISS does not use SIMD instructions to go from the 8-bit, 6-bit, 4-bit domain to the <code>float32</code> domain. This leads to poor performance in both architectures (>6x slower than Zen4).</p><h3 id=raw-distance-calculations-1-to-many>Raw Distance Calculations (1-to-many)<a hidden class=anchor aria-hidden=true href=#raw-distance-calculations-1-to-many>#</a></h3><p>We ran a standalone benchmark of L2 distance calculations to eliminate possible artifacts and overhead introduced by vector systems. Here, we used randomly generated <code>float32</code> collections of different sizes and dimensionalities. The collections range from being small enough to fit in L1 and large enough to spill to DRAM. Our code is as simple as it can get: Put the vectors in memory and do 1-to-many distance calculations with the L2 kernels taken from SimSIMD. Here, we do not do a KNN search; we only do pure distance calculations. We repeat this experiment thousands of times to warm up the caches.</p><p><strong>NEON vs NEON</strong><br>When using NEON kernels, <strong>Graviton4 is, on average, 10% faster than Graviton3</strong> across all settings. This improvement is on par with the increase in clock frequency between both microarchitectures.</p><p><strong>SVE vs SVE</strong><br>When switching from NEON to SVE, <strong>Graviton3 saw a 37% performance improvement over its NEON counterpart</strong>. <a href=https://ashvardanian.com/posts/simsimd-faster-scipy/#tails-of-the-past-the-significance-of-masked-loads>Ash Vardanian</a> also reported similar findings. However, Graviton4 doesn‚Äôt find any benefits when using SVE (in fact, compiling FAISS to SVE or not yields the same performance). Actually, SVE is <em>slightly</em> slower than NEON in G4. These could be fluctuations/noise of the benchmarks or the overhead of masked operations in SVE.</p><p>When comparing G3 SVE vs G4 SVE, these are the results across scenarios:</p><figure class=align-center><img loading=lazy src=/g3-vs-g4/gravitons-purescan.png#center></figure><p><strong>Graviton3 is, on average, 31% faster than Graviton4.</strong></p><p>You can check these fully disaggregated benchmarks for each collection size and dimensionality on <a href="https://docs.google.com/spreadsheets/d/1kUu96o_vc_-aAEBhA4_-WLjOL1AIRdDzlWQZmrl7Wps/edit?usp=sharing">this spreadsheet</a>. We would like to bring forward a few things regarding these benchmarks: (i) The wider the vectors, the bigger the gap between G3 and G4. (ii) If the vectors fit in the cache, Graviton3 can be almost 2x faster than Graviton4. (iii) Only at a dimensionality of 8, the tables turn, and Graviton3 is slightly slower than Graviton4.</p><p>The benchmarks are clear: <strong>if you are doing vector search with a vector library that supports SVE, you should use Graviton 3. It is cheaper, and it will also deliver more raw performance in the majority of scenarios.</strong></p><h2 id=why-did-graviton4-regress-the-sve-register-width>Why did Graviton4 regress the SVE register width?<a hidden class=anchor aria-hidden=true href=#why-did-graviton4-regress-the-sve-register-width>#</a></h2><p>While we do not have a proper answer to this question, we can speculate.</p><p>Currently, most code for ARM is written in NEON, where Graviton4 is better than Graviton3. A possibility is that AWS went for a CPU that performed better in existing workloads and benchmarks, which usually use code written in NEON. In fact, <a href=https://lemire.me/blog/2024/07/10/benchmarking-arm-processors-graviton-4-graviton-3-and-apple-m2/>Daniel Lemire</a> and <a href=https://chipsandcheese.com/p/arms-neoverse-v2-in-awss-graviton-4>Chips and Cheese</a> benchmarks on Graviton4 vs Graviton3 used NEON code. In other words, <strong>SVE is not yet widely used.</strong></p><p>Another possible reason is that SIMD instructions can be fully pipelined in many workloads. However, similarity calculations are different because there is a dependency chain between multiple calls to SIMD instructions. The latter benefits wider registers, especially on vectors of high dimensionalities. While we haven‚Äôt done benchmarks on SVE for other types of workloads, our intuition is that if the workload can be fully pipelined, it would be faster in Graviton4.</p><h2 id=closing-remarks>Closing Remarks<a hidden class=anchor aria-hidden=true href=#closing-remarks>#</a></h2><p>I have found myself inside a (nice) rabbit hole of CPU microarchitecture design and performance. We have actually done the same experiments presented in this blog post for 5 microarchitectures (Intel SPR, Graviton3, Graviton4, Zen3, and Zen4). One of our most interesting findings is that <strong>carefully choosing the microarchitecture for your vector search use case can give you up to 3x more queries per second and queries per dollar</strong>. This is the case, for instance, with Zen4 in IVF indexes compared to Intel Sapphire Rapids (despite the latter being a CPU with better specs). I am writing a blog post about that, so keep tuned!</p><p>Regarding the Gravitons, it is nonetheless a weird decision to halve the SIMD register size in the generational jump from Graviton3 to Graviton4. Perhaps in terms of the engineering design of the core, it is hard to keep supporting NEON registers of 128 bits AND SVE registers of double the size.</p><p>Not so far ago, Daniel Lemire commented that &ldquo;<a href=https://x.com/lemire/status/1889150598001422645>AMD is mopping the floor with ARM in terms of SIMD</a>.&rdquo;<blockquote class=twitter-tweet><p lang=en dir=ltr>To my knowledge, the best NEON offers is 4 x 128-bit, so four 128-bit instructions per cycle.<br><br>AMD is at 4 x 512-bit... and that's AVX-512, so much more powerful than NEON.<br><br>So on SIMD... AMD is just mopping the floor with ARM.<br><br>ARM has SVE/SVE2 but that does not seem to be going‚Ä¶ <a href=https://t.co/9AvOQW8lzl>pic.twitter.com/9AvOQW8lzl</a></p>&mdash; Daniel Lemire (@lemire) <a href="https://twitter.com/lemire/status/1889150598001422645?ref_src=twsrc%5Etfw">February 11, 2025</a></blockquote><script async src=https://platform.twitter.com/widgets.js></script></p><p>It is no rocket science: a smaller register will impact the performance of some workloads, even if the execution throughput is kept the same. Of course, when comparing different microarchitectures, more things come into play, such as memory access latency, read memory bandwidth, and the data-access patterns of the workload. Ultimately, <em>your</em> decision to use a microarchitecture should be based on data-driven benchmarks with your own use case. As you may have noticed, the performance depends on various factors, such as the search algorithm and the size of the vectors. Perhaps an interesting follow-up post would be to test both architectures by doing vector search under a multi-threaded setting.</p><p>For now, it seems that aside from the portability benefits, there is currently not much payoff in migrating NEON code to SVE, especially if the cores used by AWS will keep the SVE register size on par with NEON. The only exception would be when one needs to use <a href=https://ashvardanian.com/posts/simd-set-intersections-sve2-avx512/>an intrinsic that is only available in SVE</a>.</p><p>Kudos to <a href=https://ashvardanian.com/>Ash</a> for giving input on these findings and for putting them forward to the ARM team. We are still awaiting their input.</p></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/simd/>Simd</a></li><li><a href=http://localhost:1313/tags/sve/>Sve</a></li><li><a href=http://localhost:1313/tags/graviton/>Graviton</a></li><li><a href=http://localhost:1313/tags/neon/>Neon</a></li><li><a href=http://localhost:1313/tags/aws/>Aws</a></li><li><a href=http://localhost:1313/tags/vss/>Vss</a></li><li><a href=http://localhost:1313/tags/vector-search/>Vector-Search</a></li><li><a href=http://localhost:1313/tags/vector-similarity-search/>Vector-Similarity-Search</a></li><li><a href=http://localhost:1313/tags/nearest-neighbor-search/>Nearest-Neighbor-Search</a></li><li><a href=http://localhost:1313/tags/avx512/>Avx512</a></li><li><a href=http://localhost:1313/tags/avx/>Avx</a></li></ul><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share AWS Graviton 3 > Graviton 4 for Vector Similarity Search on x" href="https://x.com/intent/tweet/?text=AWS%20Graviton%203%20%3e%20Graviton%204%20for%20Vector%20Similarity%20Search&amp;url=http%3a%2f%2flocalhost%3a1313%2fgraviton3-better-than-graviton4-vss%2f&amp;hashtags=simd%2csve%2cgraviton%2cneon%2caws%2cvss%2cvector-search%2cvector-similarity-search%2cnearest-neighbor-search%2cavx512%2cavx"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share AWS Graviton 3 > Graviton 4 for Vector Similarity Search on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fgraviton3-better-than-graviton4-vss%2f&amp;title=AWS%20Graviton%203%20%3e%20Graviton%204%20for%20Vector%20Similarity%20Search&amp;summary=AWS%20Graviton%203%20%3e%20Graviton%204%20for%20Vector%20Similarity%20Search&amp;source=http%3a%2f%2flocalhost%3a1313%2fgraviton3-better-than-graviton4-vss%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share AWS Graviton 3 > Graviton 4 for Vector Similarity Search on reddit" href="https://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fgraviton3-better-than-graviton4-vss%2f&title=AWS%20Graviton%203%20%3e%20Graviton%204%20for%20Vector%20Similarity%20Search"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share AWS Graviton 3 > Graviton 4 for Vector Similarity Search on facebook" href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fgraviton3-better-than-graviton4-vss%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share AWS Graviton 3 > Graviton 4 for Vector Similarity Search on whatsapp" href="https://api.whatsapp.com/send?text=AWS%20Graviton%203%20%3e%20Graviton%204%20for%20Vector%20Similarity%20Search%20-%20http%3a%2f%2flocalhost%3a1313%2fgraviton3-better-than-graviton4-vss%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share AWS Graviton 3 > Graviton 4 for Vector Similarity Search on telegram" href="https://telegram.me/share/url?text=AWS%20Graviton%203%20%3e%20Graviton%204%20for%20Vector%20Similarity%20Search&amp;url=http%3a%2f%2flocalhost%3a1313%2fgraviton3-better-than-graviton4-vss%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share AWS Graviton 3 > Graviton 4 for Vector Similarity Search on ycombinator" href="https://news.ycombinator.com/submitlink?t=AWS%20Graviton%203%20%3e%20Graviton%204%20for%20Vector%20Similarity%20Search&u=http%3a%2f%2flocalhost%3a1313%2fgraviton3-better-than-graviton4-vss%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><script defer src=https://static.cloudflareinsights.com/beacon.min.js data-cf-beacon='{"token": "db8db92e5ccd40578410af48052268ed"}'></script><footer class=footer><span>&copy; 2025 <a href=http://localhost:1313/>Leonardo Kuffo</a></span> ¬∑
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>